关于几种梯度下降: 

看到的分类主要包括Batch-GD, Stochastic-GD, Mini-batch三种方式，其区别在于：

1. Batch-GD每次修改weight之后都要对所有的训练数据集进行重新计算，直到收敛为止；
２．可以看出BGD可能会很慢啊，对于大数据集合的训练，需要花很多时间,因而随机梯度下降就是每次只使用一个样本来进行迭代，但是
这样也存在问题，比如容易陷入局部最优解，这种也叫online training；
3. 作为一个trade-off的解决方案就是mini-batch，就是每次用一部分的数据集,当数据有很多redundancy的时候，mini-batch
的方法很好。

在训练中还有一些dark art: 
1. shifting the inputs to make the mean to be 0. 
2. do normalization to make the 