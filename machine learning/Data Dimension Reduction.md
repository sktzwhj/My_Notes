#Data Dimension Reduction Methods

最近工作中需要永道数据降维，快速学习了一下。

##线性降维
e.g., 如果是二维数据[x1,x2]，我们想把它降到１维，那么我们其实是想找到一个线性函数(一个矩阵)使得[x1,x2] = z*v = z*[v1,v2]
这里z是降维之后的结果，[v1,v2]是转移矩阵。


###1. PCA
PCA基本上是依赖于协方差矩阵的，协方差矩阵描述了数据不同维度之间的相关性。协方差矩阵的对角线是每一个维度的自相关性。那么既然是主成分分析，就是把那些相关的成分滤出，那么我们要做的就是让协方差矩阵上那些非对角线元素尽可能为0。因而可以对协方差矩阵进行对角化，对角化之后找到那些新的协方差矩阵中较大的值，这些值所对应的线性变换实际上就是将原来的特征进行线性组合所组成的相对线性无关的特征。

###2. 矩阵奇异值分解
奇异值分解的本质是找到一个向量来投影数据点，从而使偷因后的点和原来点的error只和最小。

###2. Auto Encoder