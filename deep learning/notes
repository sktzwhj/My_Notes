这是读http://neuralnetworksanddeeplearning.com/chap1.html的一点笔记。 这本书确实比我之前读过的关于机器学习的书都要更好一些。 

免得机器学习学好几次都没有一个清晰的框架。 

一直比较迷惑，机器学习、神经网络、深度学习之间的关系。 
最近看了一些说法，简单总结一下， 机器学习是人工智能的一种实现方式。 神经网络又是机器学习中的一种方法。 这不禁让人想到什么是其他的方法呢？简单来看还有
支持向量机， 线性回归， 逻辑回归， 朴素贝叶斯等等。 

和传统的统计学方法相比， 机器学习的目标是求出一个模型，可以理解为一个函数。 而统计学则是已有的模型， 通过输入数据来推演结论的过程。 

1. 感知器
其实就是神经网络当中的基本单元。 
感知器可以是那种描述线性关系来做决定的。输出要么是0要么是1。 

2. sigmoid neuros
神经网络的单元也可以是sigmoid的， 其相比于感知器的优势在于受到weight绕动的影响较小，因此在训练的时候比较好。 
输出可以是各种值。其实和感知器很类似，对于很大的输入一般输出接近1, 对于很小的输入， 输出接近0. 

一个有意思的问题是， 我们在求神经网络当中那些weight的时候， 为什么不直接求cost function的最小值所对应的每一个w呢， 因为weight恰好是这个cost function
的变量啊。但是这个变量有可能有好几亿个，面对这种情况， 这种计算恐怕是不合理的。 

Nov 7, 2016

关于backpropagation algorithm 
为了不陷入数学的坑里，就找了一个直观的解释。 
https://www.zhihu.com/question/27239198
感觉知乎上这个回答非常贴切。 其实后向传播的本质就是复合函数求导的链式法则。问题在于直接采用链式法则正向计算会有很多冗余，因而就从后往前传递链式法则当中
的中间计算结果。 

关于交叉熵代价函数， 简单的代价函数， 比如(y - a)^2/2 可能会使得学习速度非常慢， 为什么呢？
因为有的初值可能会使得对代价函数求偏导数的值变化非常慢， 因此学习速度也就很慢。 
为了解决这个问题，交叉熵代价函数被提出。 
C = -1/n sum(yln1 + (1 - y)ln(1 - a))
交叉熵之所以可以作为代价函数， 有两点非常重要。 一是非负， 二是当输出a与期望y非常接近时， C = 0. 
而且通过一系列数学计算可以得到：实际上如果我们采用交叉熵代价函数，学习速率会和误差成正比。这样神经网络就更有用了。　

关于完备性理论，为什么神经网络可以表示所有函数。
需要注意：神经网络并不能精确地计算每一个函数的值，我们只能得到我们觉得还不错的拟合。要提高拟合的精度，则需增加hidden layers.
此外，神经网络所描述的函数一般是连续函数。也就是说，如果一个函数存在很多的跳变，那么其并不适合用神经网络来进行模拟。　

