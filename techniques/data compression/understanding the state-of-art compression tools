时间: 2016年12月5日

之前关于huffman coding中频繁出现的字符可以获得更短编码的现象，我们设计了一个基于文件或者chunk的histogram skewness来对数据进行重新排序进而压缩的
方法。通过实验发现，我们的方法并不会带来什么有效的压缩率提升，其根本原因是我们对现在的数据压缩工具的本质了解还不够深入。

具体来说，压缩算法通过游程编码还有字典就会使得原来数据当中字符出现频率所具有的特征被转化得面目全非，那么我们的启发式策略就显得没有什么意义了。  

后来又考虑是不是可以通过提高动态自适应区间编码的预测效率，也就是说，我们将数据按照binary中0或者1出现的概率，从而对数据进行重拍，这样，区间编码基于之前
出现的数据对0和1概率进行估计的方式就会更加有效。但是实际上，这个粒度会非常的细，再者，0和1出现的概率本身在较粗粒度的数据分块上几乎没有什么明显的差异性。 
而直接按照预测采用的数据粒度又会是一个不可接受的计算开销。。。

总之，到目前为止，这条线索似乎是断了。 