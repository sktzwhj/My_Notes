The big picture of my topic could be like this.

The metadata management of dedup system is a challenge. Since there might be a lot of features. In this case, it can be a bottleneck. So it is important 
to find some methods to efficiently manage the metadata. 

an important thing is the routing of the data. (recipe?)

Alternatives for achieving better compressibility. 
(1) migratory v traditional compression 
mc has larger window size to let find more potential similarity 
(2) delta compression
encode A' ralative to a similar object A 

Approach of mc

In dedup or compression systems, a method to restore the original data is necessary.

Similarity detection with super-features. 
(this is widely used in identify similar web pages, files and chunks within files).

each time a file is written, its N SF features would be looked up in N hash tables. 

In 3.1.1, the authors mention 'large file has larger fingerprints thus larger metadata'

The recipes used in mc is migrate recipe and restore recipe. 

What are the SFs?
--this question is answered in the techniques-> file systems -> delta compression

Overall, the overhead introduced is not too large and the improvement of the compressibility is quite considerable. 

[log 09/06/16]
actually, I find my former topic is very consistent with the current one. The most important part in my former proposal is 
talking about write amplifications in LSM-tree. The current one is just talking about the redundant data in storage systems. 
To this end, I think it is not very far away. 

[log 09/06/16]
can we cache the assignment result on client side, and introduce some methods to let the fix the errors. Actually, in 
backup scenario, the result of assignment can be cached in the clients. Because those data is rarely deleted. 

[log 10/06/16]
in single-node dedup systems, the challenge of both providing high throughput and good dedup ratio is solved by choosing a proper 
chunk size. 

However, things are different in distributed storage. another thing is how to keep the load balancing. 

so single or multiple metadata servers is actually a problem when we are designing a distributed storage with dedup. 
