Authors: Mengbai Xiao et al. from George Mason U, NetApp..
Published in HotStorage'15
Date: May 24, 2017

这篇文章就是说把纠删码和去重结合在一起可能会导致一些问题。具体来说，如果简单地先去重再做纠删码，主要问题是去重会把很多连续读变成随机读，这对于像HDD这样
的磁盘来说是个严重的问题。

所以作者还是不希望对数据块进行去重，那么相应地，我们在校验块当中可能存在一些去重的机会。

预处理阶段：先进行基本的去重，不过这里不用on-disk fingerprint table，只用cache。通过基本去重可以标记每个文件的重复位置。

减小校验块的大小，这里的方法叫做pseudo shuffling，具体来说，unique的块应当被一起校验。有duplicate的貌似就不用校验了，因为有重复不就说明有了备份？
好像是这个意思。 

但好像只有说duplicate的其他copy和当前copy在不同的node上才有用，不然怎么来恢复呢？所以文章中作者其实也对块进行了重排序。

