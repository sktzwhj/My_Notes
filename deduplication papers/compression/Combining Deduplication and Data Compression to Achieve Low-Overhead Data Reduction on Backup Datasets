Author: Wen Xia et al. from HUST
Published in 2014 Data Compression Conference
Date: 19/06/2016

Deduplication 是用来消除冗余的，前提是发现两个chunk之间存在重复

Delta Compression　是用来压缩的，不需要发现chunk之间的重复，只要发现chunk之间有很大的相似性就可以

所以说，如果在dedup之后进行delta compression，实际上可以获得进一步的压缩．因为dedup并不能消除所有的冗余

但是如果要进行delta compression，重要的就是如何找到那个base，因为delta compression总是需要base的．因而本文提出一种
相似性的检测方法．

因为dedup要找到是完全相同的冗余部分，而delta-compression找的是相似的，这就使得相比之下，后者想要做到很好的扩展性就很
困难了．

本文提出的系统DARE的系统流程是这样的.

(1) dedup detection
本文提出一个DupAdj的检测方法．
用一个双向链表来标记chunk．


(2) 相似性检测
这一步主要用来检测出在第一步检测中没有检出的项目
本文采用基于super feature的方法来匹配，可以简单的理解super feature就是对feature进行rabin fingerprint. 但是如果
用来计算super feature的sub features数量过多，就会使得匹配的identical items的数量更多，如果比较少，找到的相似就更多．

(3) Delta Compression
(4) 存储管理

文章实际上是对super feature的应用进行了一些修改，

不过我的理解貌似还有点问题，从figure2 来看，貌似是说如果一个chunk是duplicate的，那么它附近的chunk就有很高的可能性是
similar的，这个性质貌似对于backup的存储还是成立的，但是对于inline的话，正在写入的数据和以前的数据的关系可能就没有那么
紧密了．

那么有没有什么应用是能够确保这一点呢？正在写入的数据和之前的数据关联很紧密．

可是为什么DARE会比单纯的dedup的restore时间更快呢？

可能的原因包括局部性更好，cache更有效，需要读的container的数量更少．