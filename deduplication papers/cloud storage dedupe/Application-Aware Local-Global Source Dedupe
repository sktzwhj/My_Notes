Authors: Yinjin Fu,  Hong Jiang, Nong Xiao et al.
Date: Oct. 6th, 2016
Published on TPDS in May, 2014. 

This paper tries to solve the problems in personal dedupe. That is, the devices like mobile phones, etc.

Dedupe is a resouce-intensive process. The resources here includes the CPU resource and I/O resouce to identify the duplicates. (This can also be used in our paper)

the idea : combine the limited resource but low-latency client-side resource and high-latency but powerful WAN resource.

So basicly, the idea of this paper is not complicated. It is based on some usual observations like: 

(1) Large files take up the majority of space. 
(2) The shared redundancies among the data of different applications are very rare. 
(3) The optimal combination of chunking and hash fingerprintting method can reduce system overheads. 
(4) For different applications, different chunk sizes should be used...

So basicly, we can do the dedupe for large files at the side of client while perform dedupe on tiny files at the cloud side. 

Not bad...