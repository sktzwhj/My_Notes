Author: Min Fu
Published in: FAST'15
Date: 03/06/2016

This paper is an extensive study of the tradeoffs in designing the deduplication performance in backup workloads. 
Their framework is open-source. https://github.com/fomy/destor

They mention that typical deduplication system has three parts. (1) fingerprint index (2) recipe store (3) container store. 

1. fingerprint index 
the simplest finger index is key-value store. (key, vlaue) -> (fingerprint, index to chunk)

the categories of: ED (Exact Dedup), ND (Near-exact Dedup), LL (Logical Locality), PL (Physical Locality)

ED (Exact Dedup), ND (Near-exact Dedup)
generally, near-exact dedup costs more. 

LL (Logical Locality), PL (Physical Locality)

The difference between LL and PL is that for the later, the recipe store stores the container id as values, while for LL, the segment id is stored in 
values. 
Although physical locality is an effective approximation of logical locality. However, as the system ages, there would be fragmentation.

(How to understand fragmentation)


The segmenting method,
fixed-size or content-defined. 

Sampling method, 
uniform, random, minimum
(Why we can compare the similarity by comparing the similarity of fingerprints?)

Segment selection,
top-K

Segement prefetching,
segement prefetching can partially alleviate the problem caused by boundary change. (i.e. because even boundary has changed, we can still read the adjacent
data in the cache)

Key-Value mapping relationship
actually, one key can map to several values, that is, maybe the feature is shared by several segement. (i.e. different versions of a segement, corruputed data
is restored or recovered to old version.)


The performance of restore decreases due to the fragmentation. 

via efficient rewriting algorithm, the lookup overhead no longer increases over time. 

What is restore doing? What is rewrite doing?
The rewrite phase identifies fragmented deduplicate chunks and rewrites them to improve the restore performance. 

What does self-reference mean in VMDK workload?
That means in VMDK workload, the difference between snapshots is not tht big, say we maybe only have few differences. We call this self-reference. 


What does the sampling-ratio mean?
Since it is not practical to calculate all fingerprints of a segment, it is a proper approximation to calculate some sample fingerprints. 

What is the difference between container and segeent?
unique chunks of a backup are aggragated into containers. This is for physical locality. However, for logical locality, the locality is reserved 
in recipe, each recipe is divided into subsequences called segments. 

The conclusions, 

(1) While the fragmentation results in an ever-increasing lookup overhead in EDPL, EDLL achieves sustained performance. The sampling optimization
performs an efficient tradeoff in EDLL.
(2) In NPDL, the uniform samling is better than random sampling. The fingerprint cache has minimal impacts on deduplication ratio. 
(3) The base procedure underperforms in NDLL if self-reference is common. Reading most similar segment is not enough due to self-refernece. 
(4) If self-reference is rare, the Base procedure is sufficient for a high deduplication ratio. 
(5) If self-reference is common, the similarity detection is necessary. The segmenting prefetching is a great complement to Top-k. 
(6) The rewriting algorithms helps EDPL to achieve sustained backup performance. With a rewriting algorithm, EDPL is better due to its higher 
    deduplication ratio than other index schemes. 
(7) Although near-exact deduplication reduces the DRAM cost, it cannot reduce the total storage cost. 
    
What on earth is the restore algorithm?

