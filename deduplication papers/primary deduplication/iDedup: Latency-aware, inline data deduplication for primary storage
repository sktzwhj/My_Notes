Author:Kiran Srinivasan, et al. from NetApp, Inc. 
Published in FAST'12
Date:10/06/2016

The work before this paper does not use dedup inline for latency sensitive primary workloads. The main reasons are (1) 
deduplicating data on disk cause fragmentation that increases seeks for subsequent sequential reads of the same data. 
(2) dedup data requires extra disk I/O to access on-disk dedup metadata. 

The purposes of iDedup is to provide primary inline dedup. Meanwhile, minimizing the extra seeks and I/O

the algorithm of this paper is based on two insights:
(1) spatial locality exists in duplicated primary data. 

so only performing deduplication when a sequence of on-disk blocks are duplicated. 

(2) temporal localisty exists in the access patterns of duplicated data

maintain an in-memory fingerprint cache to detect 

Since most dedup work occurs in the write path, it becomes even harder to maintain the already slow wirte latency. 

And in order to dedup, some extra IOs are necessary to identify duplicates. 

So the first matigate the fragementation caused by inline dedup. In addition, the second removes extra I/Osand lowers
write path latency. 

The classification for dedup systems. 

primary systems and secondary systems. 
can still be divided into inline and offline. 

Challenges for primary deduplication:
write path: metadata management and IO requried to perform dedup inline with the write request increases write latency. 
read path: the fragmentation of otherwise sequential writes increases the number of seeks required during reads. this also
increases the latency. 
delete path: the requirement to check whether a block can be safely deleted increases delete latency. 

The methodologies of this paper:
(1) to leverage spatial locality, attempt to only dedup full sequences of file blocks if and only if the sequence of blocks
are a)sequential in file b) have duplicates that are sequential on disk. So if we enforce the minumum sequence length for 
such sequences, the extra seek cost is expected to be amortized .

(2) to leverage temporal locality. the mapping of (fingerprints, data block addresses) is regarded as metadata, so we can 
use LRU cache to cache this metadata. 

Some details:

(1) caching the fingerprint metadata may have some negative effects on cache for data 
(2) how to find a sequence of duplicate? i.e., if the sequence length threashold is 2, we calculate the Hash of B(n) and 
B(n+1), that is, H(n) and H(n+1), if H(n) and H(n+1) exist in fingerprint table and the block addresses of H(n) and H(n+1) 
is continous, then we are done. 

Brief analysis of experimental results. 

Compared to smapling methods, it seems that iDedup still calculates all the hash values. But actually, for a primary inline 
dedup, it is not a good choice to use batching and sampling. Because these may introduce some latency. So that is why only 
log-structured file system is suitable. 

Here, as NVRAM is used as the write log, the data would not be lost. consistency is guaranteed. 