Author: Bo Mao et al. from Xiamen U
Published in IPDPS'14
Date: 01/07/2016

This paper is for deduplication in primary storage systems. The focus of this paper is that small 
data I/O is the performance bottleneck for cloud storage. System like iDedup focuses on the capcity.
However, this paper looks more on performance. 

Using deduplication of backup storage systems to primary storage systems directly would cause two 
problems: (1) overhead of memory indexing. (2) fragmentation

Actually, there are two kinds of cache in the context of deduplication. 
(1) index cache which is used to accelerate the performance of writing because most fingerprint 
comparisons can be finished in memory. However, the read memory is used to accelerate the performance
of reading the content of data. Therefore, since the total number of memory is limited, these two
become a contradictory. 

The solutions of this paper:

(1) Select Dedupe
The module classifies the redundant write into three types
a) fully redundant write requests whose write data are already sequentially stored on disks. 
b) partially redundant write whose write data are a mixture of redundunt and unique data. 
c) the partially redundant write of which the number of redundant data chunks per request exceeds 
the threshold. 

a) and c) are deduped. the main purpose is to maintain a relatively high performance for read. 
Because if we do dedup on b), the data would be stored dispreadly, so the fragmentation would 
significantly influence the read performance. (note that sequ)

(2) iCache
We can regard this as a cache manager. This is very similar to what we want to do. The basic idea
is to calculate the cost-beneift value of two cache( index (fingerprint) or read cache ). Therefore, 
the  more hits a cache can get, the larger size the cache should be. 