Author: Feng Chen, etc. 
Published in FAST'11
Date:08/06/2016

The main concern of this paper is that the lifespan of SSD devices is much shorter compared with HDDs. So the authors solve 
this problem in the FTL. By reducing the duplicate writes to SSDs, the lifespan increases. 

An interesting reference can be, although the bit density of SSD grows, the raliability of SSDs is even worse. 

the methods used in this paper: 

(1) making FTL content aware. 
some interesting ideas. e.g., log-like write mechanism makes it possible to re-validate the data rather than re-writing. 

CAFTL is a combination of both in-line and out-of-line dedup. 

fixed-size chunking rather than variable-size chunking is used. because in SSD, the mapping and many other mechanisms are
designed for page size. 

A discovery is that most fingerprints cannot be matched. Most comparisons are useless. 
To solve this problem, 

the authors partition the hash value space into N segements. so a fingerprint can be assigned to a certain segement by 
hashing (e.g., f mod N). each segment contains some buckets. each bucket is a 4KB page in memory. each reference has a 
8-bit reference counter. 

when the size of fingerprints is very large, then comparing the fingerprints is very time-consuming. 

indirect mapping. the existing 1-to-1 mapping mechanism in SSDs cannot be directly used for CAFTL, which is essentially N-to
1 mapping. the challenge is (1) the physical page is relocated to an other place (by GC, for example). we must efficiently 
mapping all logical pages mapped to this page to its new physical page. (2) the physical page cannot be collected before 
all the logical pages referencing it has been demapped. so that we must track the reference count. 

in the paper, the authors introduce two-level indirect mapping. the benefit of keeping the direct mapping with indirect mapping 
is that we can guarantee the performance of regular workload. the fingerprints are stored in the memory in the SSD. 

the reason for sampling for hashing. because most data in file system workload is not duplicated. So comparing the fingerprint 
is a waste of time in most cases. Therefore, only a sample fingerprint is chosen to be compared, if that fingerprint is duplicated, 
we then calculate all the fingerprints to compare. 

To this end, the most important thing is how to choose a sample. in this paper, it is unwise to rely on hashing to choose 
the samples because hashing is time-consuming. The authors just choose the first 4 bytes. 

In addition, the pre-hashing only uses CRC rather than SHA to save computing time. 

Since the in-line dedup may hurt the performance of regular workload. So high/low watermark are used to indicate if the 
in-line dedup should be stopped. The data left would be processed in the out-of-line dedup. 

The out-of-line dedup can be processed together with the GC. And it can be interrupted by the normal requests to ensure the 
performance of foreground jobs. 

Let's learn how to use SSD simulators. 

Critical Thinking about this paper:

