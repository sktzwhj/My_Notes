Author: Avani Wildani et al. from UC Santa Cruz
Published in ICDE'13
Date: 12/06/2016

The idea of this paper seems very similar to the FAST'13 paper 'improving restore speed...'?

The core idea of this paper is to cache the working set in the cache. So can store less index data in the fingerprint thus
improving the efficiency. 

In order to avoid the fingerprint index to be the performance bottleneck of primay system, the fingerprint indices should 
be stored in the memory. 

However, memory is much more expensive compared to disk. To this end, there is an example in the paper that if the dedup 
ratio is less than 45%, we would actually pay extra money to save the fingerprint indices. 

Major contributions of this paper:
(1) use a dynamic , scalable method to select a portion of the dedup index to store in the memory. 
(2) evaluation 

Working set identification
working sets are groups of block addresses or offsets that are likely to be accessed together. 

The tradeoff in this paper is about the size of working set and space-efficiency and the time needed for calculation. 

So a description about this method is like the followings, 
(1) initilization 
creating the working set table. initialize the cache with the most hot working set. 

(2) dedup
if a match is found in the memory, we just dedup the data as usual. Otherwise, on a cache miss, the request is sent to the 
on-disk fingerprint cache. Maybe bloom filter or something else is here. And after we find the match on disk, the corresponding 
working set is moved to the cache in memory. 

(3) new data
the new data does not belong to any working set, so it is placed in a temporary area. The next time when working set is 
being computed, it would be added to a working set. 

