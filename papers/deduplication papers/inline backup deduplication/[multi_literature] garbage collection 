Luckily, I discover the blog of Min Fu from HUST. 

So it seems that I can learn a lot of early literature more fast...

Let's talk about garbage collection in deduplication...

Garbage collection can be summarized as two phase... the first is to mark the space which needs to be collected, the second is to collect 
that data. 

For the first phase, there are a bunch of literatures...

1. Easy one, like reference count...
2. Scan recipe, this is an offline method. scan the recipe and the fingerprints, the data only exist in fingerprints can be collected. 
3. Bloom filter, compared to 2, it saves some space. However, it introduces some errors...
4. Bit vector, it seems to use bits to indicate whether a data block is used. But the real position of the data block is defined in the 
   offset of the container... so if use this method, the time for building the vector would be long...
5. Perfect hashing. 
6. Bitmap compare. This is an online method, a bitmap indicates in a backup, which blocks are referenced. So when combine all the bitmaps, we can 
   find the blocks which are not referenced now. 
   
   
Related papers,
[1] Wei et al. MAD2: A scalable high-throughput exact deduplication approach for network backup services. MSST’2010.
[2] Guo et al. Building a high-performance deduplication system. USENIX ATC’2011.
[3] Botelho et al. Memory Efficient Sanitization of a Deduplicated Storage System. FAST’2013.