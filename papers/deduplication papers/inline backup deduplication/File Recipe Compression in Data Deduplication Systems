Author: Dirk Meister et al. from Johannes Gutenberg Univ. Mainz
Published in FAST'13
Date:13/06/2016

Main idea: In dedup systems, if the dedup ratio is very high, the size of recipe would be very large. To this end, this paper uses
a compression method to compress the recipes. (by up to 93%).

Seems that duplicated data existed in the metadata of deduplication itself. 

a file recipe contains a list of chunk identifiers. 

The structure of file recipe is like (file id, fingerprints), so if the fingerprints are very long, it would waste a lot of space. 

Specific methodologies, 

(1) zero-chunk suppression 

zero chunks are very common in VM disk images. also, this method can be used to reduce the size of file recipes. 

(2) chunk index page-oriented approach

this approach aims to assign a code word to all chunks that is not significantly longer than necessary to have a unique code word for
each chunk in the system. 

the code word consists of prefix and suffix. 

There are some entropy knowledge used in this paper. The most used fingerprints could be assigned a short word code. 

Then it becomes a problem how to get the frequency of a certain chunk? the reference number is very suitable to be used in this scenario. 
although this statistic is initially collected for garbage collection. 
(3) the statistical method

the entropy of a fingerprint is -log2(usage count of h/total chunk usage).

we assign a shorter code word for longer fingerprints. Although it is obvious that Huffman tree seems to be a proper algorithm. However, 
there are some limitations prevent it to be achieved. In this paper, Misra-Gries stream algorithm is used. 
