Topic: Improving the Data Compressibility in Primary Storage Systems

Talk for PhD Boot Camping. 

Today I would like to talk about my PhD topic which is related to improve the compressibility of 
primary storage systems. The compressibility means how we can reduce the volume of data. Primary 
storage systems are relative to backup or archival storage systems. 

We all know that we are generating more and more digital data today. But an interesting trend is 
that most of them are duplicates. In backup storage, the duplicate ratio can be around 90%. Even in 
primary storage system, the number is about 60%. So it is important to remove the duplicates. Maybe
some of you would say that hard disks are very cheap today, but the purpose of doing so is far more than 
just saving storage devices.  Firstly, eliminating unnecessary redundant data storage. Secondly, making
it easier to retrieve similar data content. Thirdly, increasing the lifespan of storage devices 
like solid-state device (SSD) by reducing the total number of writes.

Currently, there are two main techniques to improve the space efficiency in storage: data 
deduplication and data compression. The typical method of data deduplication is like this. Every time 
we write a data into the storage, we partition it into chunks, we calculate the hash value of the 
each chunks and store these hash values as fingerprints. 
Next time we write a data, we check the fingerprint indexes to see if the data has already been written. 
But as you can imagine, the computation and indexing overhead of deduplication are very notable. As 
a result, it would significantly reduce the throughput or latency of the storage access operations. 
Therefore, although a lot of reserch has been done in this area for over 10 years, it is still only 
good for backup storage systems. For primary storage, the demand for latency is critical, so it is
not easy to adopt this kind of deduplication. Another technique is data compression, especially delta
compression. Compression could find fine-grained duplicates which might be smaller than the size of 
a chunk. Compression techniques find duplicates in a window. The larger the window is, the more duplicates
can be found. But of couse, the computation overhead increases.  Recently, data compression is also 
being combined with deduplication in some systems. But the limited-sized window makes current compression
techniques miss a lot of potential duplicates. 

So we are thinking about whether we could further improve the performance and reduce the overhead of 
current data deduction techniques. We envsion that the data deduction techniques should consider the 
features of certain applications. For instance, we discover that when some online photos are grouped 
by their tags, we can get more deduplication gain. And in file systems, since the metadata like superblocks
are originally duplicated for reliability, this kind of hint can be used to help removing some useless
deduplication thus saving some computation resources. 

And from software engineering perspective, we are looking for a method to elegantly transfer application
level information to the storage systems for better compatibility and scalability. 


